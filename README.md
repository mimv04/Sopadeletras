### 7. Supervisión algorítmica y auditorías de sistemas de IA

**Contribución de 529677**

**Contribución de 20380**
El tema de la supervisión algorítmica y las auditorías de sistemas de IA es uno de esos que nos hacen pensar en el futuro. A medida que la inteligencia artificial se usa más en nuestro día a día y en las empresas, la responsabilidad que debemos tener con ella aumenta significativamente. La IA ya no es un concepto lejano o fuera de nuestro alcance; al contrario, cada día nos encontramos con más programas, tecnologías y hasta electrodomésticos que la incorporan. Está por todos lados: desde la recomendación de una serie en Netflix hasta decisiones médicas o financieras. Ante esto, surge una pregunta clave: ¿cómo nos aseguramos de que estos sistemas funcionen correctamente, sin causar daños o ser discriminatorios? [^23]

Para responder a esa pregunta, hablamos de dos frentes de control muy importantes: la supervisión algorítmica y las auditorías de IA.

**¿Qué es la supervisión algorítmica?**
Imagina que es como tener a alguien vigilando constantemente lo que hace la IA. Es un seguimiento continuo que permite detectar errores o sesgos antes de que causen problemas. Los algoritmos pueden cambiar su comportamiento con el tiempo, sobre todo si aprenden de datos nuevos. Por eso, es fundamental monitorear su desempeño constantemente y corregir cualquier alteración o desviación que pueda llevar a resultados injustos o incorrectos. Esta tarea no solo requiere conocimiento técnico, sino también una comprensión ética y social del contexto en el que opera la IA.[^25]

**¿Por qué es tan importante la supervisión?**
La supervisión es indispensable porque los algoritmos de aprendizaje automático, en especial los más complejos como las redes neuronales profundas, a veces son como una "caja negra". ingresas datos y obtienes un resultado, pero no siempre es fácil entender por qué se llegó a esa conclusión. Es ahí donde pueden surgir sesgos ocultos. Por ejemplo, un algoritmo de contratación podría empezar a descartar currículums de personas con nombres que suenan a cierto grupo social, o un sistema de préstamo bancario podría favorecer inconscientemente a un grupo demográfico. Si no supervisamos constantemente el comportamiento del algoritmo y los resultados que arroja, esos sesgos pueden perpetuarse y amplificarse, generando injusticias en las decisiones. Y esto no es solo una cuestión de "justicia", sino de confianza. Si la gente no confía en que los sistemas de IA son imparciales, su adopción se verá limitada, y con razón.

**¿Qué son las auditorías algorítmicas?**
Las auditorías algorítmicas son evaluaciones más profundas y formales, una especie de examen estructurado. Es un proceso sistemático para evaluar el rendimiento de un sistema de IA, su robustez (qué tan bien maneja datos inesperados o ataques), su transparencia (qué tan explicables son sus decisiones), y su equidad, responsabilidad y ética.

Va mucho más allá de simplemente mirar números; se revisan los procesos, la gobernanza y cómo se toman las decisiones sobre la IA dentro de una organización. Estas auditorías pueden ser internas, realizadas por el equipo desarrollador, o externas, ejecutadas por instituciones independientes. Buscan responder preguntas clave como: ¿con qué datos fue entrenado el algoritmo?, ¿produce resultados discriminatorios?, ¿las personas pueden entender cómo se llegó a una decisión? Si no se puede responder claramente a estas preguntas, existe un riesgo de opacidad y abuso.

Hoy en día, contamos con diferentes tipos de auditorías, como las de datos, de algoritmos, de modelos o de impacto social. Además, existen guías o marcos de evaluación de IA como el **"AI Risk Management Framework" (NIST, 2023),** que nos ayudan a llevar a cabo estas revisiones de manera efectiva.[^24] 

[^23]: UNESCO. (2021). *Recomendación sobre la ética de la inteligencia artificial* [versión en español]. [https://unesdoc.unesco.org/ark:/48223/pf0000381137_spa](https://unesdoc.unesco.org/ark:/48223/pf0000381137_spa)

[^24]: *Auditorías de algoritmos de IA: Consideraciones clave de control* (2024). [https://www.isaca.org/resources/news-and-trends/industry-news/2024/ai-algorithm-audits-key-control-considerations](https://www.isaca.org/resources/news-and-trends/industry-news/2024/ai-algorithm-audits-key-control-considerations)

[^25]: National Institute of Standards and Technology (NIST). (2023). *AI Risk Management Framework (AI RMF 1.0)*. NIST SP 100. [https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-1.pdf](https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-1.pdf)
